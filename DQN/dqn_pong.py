from lib import wrappers, dqn_model

import torch
import torch.nn as nn
import torch.optim as optim

import numpy as np
import collections
import argparse
import time
import os

from tensorboardX import SummaryWriter


# hyperparameters
DEFAULT_ENV_NAME = "Pong-v0" # if use other envs remember to check whether needs to add some wrappers
MEAN_REWARD_BOUND = 19.0 # pong game end when an agent gets 21 points

GAMMA = 0.99 # used for Bellman approximation
BATCH_SIZE = 32 # the size sampled from replay buffer
REPLAY_SIZE = 10000 # the maximum capicity of replay buffer
REPLAY_START_SIZE = 10000 # the counts of frames we wait before start training
LEARNING_RATE = 1e-4 # learning rate used in Adam optimizer
SYNC_TARGET_NET = 1000 # how frequently we sync target net

EPSILON_DECAY_LAST_FRAMES = 150000 # during the first 150,000 frames, epsilon is linearly decayed to 0.01
EPSILON_START = 1.0
EPSILON_END = 0.01

SAVE_PATH = "./"+DEFAULT_ENV_NAME+"_training_data"
SAVE_PATH_CUDA = "./"+DEFAULT_ENV_NAME+"_training_data_cuda"


"""The core of our Q-learning procedure is borrowed from supervised learning. 
Indeed, we are trying to approximate a complex, nonlinear function, Q(s, a), 
with an NN. To do this, we must calculate targets for this function using the 
Bellman equation and then pretend that we have a supervised learning problem at hand. 
That's okay, but one of the fundamental requirements for SGD optimization is that the 
training data is independent and identically distributed (frequently abbreviated as i.i.d.)


1. Our samples are not independent. Even if we accumulate a large batch of data samples, 
   they will all be very close to each other, as they will belong to the same episode.

SOLUTION:
we usually need to use a large buffer of our past experience and sample training data from it, 
instead of using our latest experience. Replay buffer allows us to train on more-or-less independent 
data, but the data will still be fresh enough to train on samples generated by our recent policy.


2. Distribution of our training data won't be identical to samples provided by the optimal 
   policy that we want to learn. Data that we have will be a result of some other policy 
   (our current policy, random, or both in the case of epsilon-greedy), but we don't want to 
   learn how to play randomly: we want an optimal policy with the best reward.

   The Bellman equation provides us with the value of Q(s, a) via Q(s', a') (bootstrapping). 
   However, both the states s and s' have only one step between them. This makes them very similar, 
   and it's very hard for NNs to distinguish between them.
   This can make our training very unstable, like chasing our own tail: when we update Q for state s, 
   then on subsequent states we will discover that Q(s', a') becomes worse, but attempts to update it 
   can spoil our Q(s, a) approximation, and so on.

SOLUTION:
we keep a copy of our network and use it for the Q(s', a') value in the Bellman equation. 
This network is synchronized with our main network only periodically, for example, once in 
N steps (where N is usually quite a large hyperparameter, such as 1k or 10k training iterations)
"""


Experience = collections.namedtuple("Experience", field_names=["state", "action", "reward", "done", "next_state"])


class ExperienceBuffer:
    def __init__(self, capacity):
        self.buffer = collections.deque(maxlen=capacity)

    def __len__(self):
        return len(self.buffer)

    def append(self, experience):
        self.buffer.append(experience)

    def sample(self, batch_size=BATCH_SIZE):
        index_mask = np.random.choice(len(self.buffer), size=batch_size, replace=False) # replace=False means no repetition
        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in index_mask]) # nice trick!
        return np.array(states), np.array(actions), \
               np.array(rewards, dtype=np.float32), \
               np.array(dones, dtype=np.uint8), \
               np.array(next_states)


"""Agent interacts with the environment and saves the result of the interaction into the experience replay buffer.
In Pong environment our agent is on the right.
"""
class Agent:
    def __init__(self, env, exp_buffer):
        self.env = env
        self.exp_buffer = exp_buffer
        self._reset()

    def _reset(self):
        self.state = self.env.reset()
        self.total_rewards = 0.0

    @torch.no_grad()
    def play_one_step(self, net, epsilon=0.0, device="cpu"):
        done_reward = None
        
        # choose action using epsilon greedy policy
        if np.random.rand() < epsilon:
            action = self.env.action_space.sample()
        else:
            state_a = np.array([self.state], copy=False) # add an dimension for BATCH_SIZE!
            state_v = torch.tensor(state_a).to(device)
            q_values = net(state_v) # since we put in state_v, batch_size=1 so q_values's size is [1, action_space.n]
            _, action_v = torch.max(q_values, dim=1)
            action = int(action_v.item())
        
        # step action
        next_state, reward, done, _ = self.env.step(action)
        exp = Experience(state=self.state, action=action, reward=reward, done=done, next_state=next_state)
        self.exp_buffer.append(exp)
        self.total_rewards += reward
        if done:
            done_reward = self.total_rewards
            self._reset()
        self.state = next_state

        return done_reward 


def calc_loss(batch, net, target_net, device="cpu"):
    states, actions, rewards, dones, next_states = batch
    states_v = torch.tensor(np.array(states, copy=False)).to(device) # copy=False means if states changes states_v will change too
    actions_v = torch.tensor(np.array(actions, copy=False)).to(device) # use copy can save memory
    rewards_v = torch.tensor(np.array(rewards, copy=False)).to(device)
    done_mask = torch.BoolTensor(dones).to(device)
    next_states_v = torch.tensor(np.array(next_states, copy=False)).to(device)

    """some explanation:
    1. gather: it's like index mask
    >>> t = torch.tensor([[1, 2], [3, 4]])
    >>> torch.gather(t, 1, torch.tensor([[0, 0], [1, 0]]))
    >>> tensor([[1, 1],
                [4, 3]])
    >>> torch.gather(t, 0, torch.tensor([[0, 0], [1, 0]]))
    >>> tensor([[1, 2]
                [3, 2]])

    2. unsqueeze:Returns a new tensor with a dimension of size one inserted at the specified position.
      The returned tensor shares the same underlying data with this tensor.A dim value within the 
      range [-input.dim() - 1, input.dim() + 1) can be used. Negative dim will correspond to unsqueeze() 
      applied at dim = dim + input.dim() + 1.
    >>> x = torch.tensor([1, 2, 3, 4])
    >>> torch.unsqueeze(x, 0)
        tensor([[ 1,  2,  3,  4]])
    >>> torch.unsqueeze(x, 1)
        tensor([[ 1],
                [ 2],
                [ 3],
                [ 4]])

    3. squeeze: Returns a tensor with all the dimensions of input of size 1 removed.
       For example, if input is of shape: (A×1×B×C×1×D) then the out tensor will be of shape: (A×B×C×D).
       When dim is given, a squeeze operation is done only in the given dimension. If input is of shape: (A×1×B), 
       squeeze(input, 0) leaves the tensor unchanged, but squeeze(input, 1) will squeeze the tensor to the shape (A×B) .
    """
    q_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)

    with torch.no_grad():
        max_next_q_values = target_net(next_states_v).max(1)[0] # maxmize along axis 1, return values and index. we need values only
        
        """Here we make one simple, but very important, point: 
        if transition in the batch is from the last step in the episode, then our value of the 
        action doesn't have a discounted reward of the next state, as there is no next state 
        from which to gather the reward. This may look minor, but it is very important in practice: 
        without this, training will not converge. next_state_values
        """
        max_next_q_values[done_mask] = 0.0

        """then detach the value from its computation graph to prevent gradients from flowing into the NN 
        used to calculate Q approximation for the next states. This is important, as without this, our 
        backpropagation of the loss will start to affect both predictions for the current state and the next state. 
        However, we don't want to touch predictions for the next state, as they are used in the Bellman equation to 
        calculate reference Q-values. To block gradients from flowing into this branch of the graph, we are using 
        the detach() method of the tensor, which returns the tensor without connection to its calculation history.
        """
        max_next_q_values = max_next_q_values.detach()

    expected_value = rewards_v + GAMMA * max_next_q_values
    return nn.MSELoss()(q_values, expected_value)


def train(env, net, target_net, buffer, agent, optimizer, device, save_path):
    print("start training")
    writer = SummaryWriter(comment='-'+DEFAULT_ENV_NAME)
    frame_idx = 0
    ts_frame = 0
    best_reward = -22.0
    total_rewards = []
    ts = time.time()
    while True:
        frame_idx += 1
        epsilon = max(EPSILON_END, EPSILON_START - frame_idx / EPSILON_DECAY_LAST_FRAMES)
        reward = agent.play_one_step(net, epsilon, device) # when play steps buffer will be populated
        if reward is not None:
            total_rewards.append(reward)
            speed = (frame_idx - ts_frame) / (time.time() - ts)
            ts_frame = frame_idx
            ts = time.time()
            mean_reward = np.mean(total_rewards[-100:])
            print("frame: %d| %d games done| mean_reward100: %.3f| eps: %.3f| speed: %.3f f/s" %\
                 (frame_idx, len(total_rewards), mean_reward, epsilon, speed))

            writer.add_scalar("dqn_"+DEFAULT_ENV_NAME+"_mean_reward100", mean_reward, frame_idx)
            writer.add_scalar("dqn_"+DEFAULT_ENV_NAME+"_epsilon", epsilon, frame_idx)
            
            if best_reward < mean_reward:
                torch.save(net.state_dict(), save_path+"/"+DEFAULT_ENV_NAME+"-_%.2f.dat" % mean_reward)
                print("best reward update: %.3f -> %.3f" % (best_reward, mean_reward))
                best_reward = mean_reward
            if mean_reward > MEAN_REWARD_BOUND:
                print("solved in %d frames" % frame_idx)
                break

        if len(buffer) < REPLAY_START_SIZE:
            continue
        
        # Don't forget sync target net otherwise it won't converge!
        if frame_idx % SYNC_TARGET_NET == 0:
            target_net.load_state_dict(net.state_dict())

        optimizer.zero_grad()
        batch = buffer.sample()
        loss = calc_loss(batch, net, target_net, device)
        loss.backward()
        optimizer.step()
    writer.close()
        

def main(to_train, save_path):
    torch.manual_seed(1234)
    parser = argparse.ArgumentParser()
    parser.add_argument("--cuda", default=False, action="store_true", help="Enable cuda computation")
    parser.add_argument("--env", default=DEFAULT_ENV_NAME, help="default env name")
    args = parser.parse_args()
    device = torch.device("cuda" if args.cuda and torch.cuda.is_available() else "cpu")

    os.makedirs(save_path, exist_ok=True)
    env = wrappers.make_env(args.env)
    net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)
    target_net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)
    print(net)

    buffer = ExperienceBuffer(REPLAY_SIZE)
    agent = Agent(env, buffer)
    epsilon = EPSILON_START

    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE) # only need one optimizer

    if to_train:
        train(env, net, target_net, buffer, agent, optimizer, device, save_path)


if __name__ == "__main__":
    main(to_train=True, save_path=SAVE_PATH_CUDA)